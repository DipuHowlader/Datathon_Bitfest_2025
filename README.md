# Datathon_Bitfest_2025

# Job Applicant Matching System

![GitHub](https://img.shields.io/badge/license-MIT-blue)
![Python](https://img.shields.io/badge/Python-3.8%2B-green)
![Machine Learning](https://img.shields.io/badge/Machine%20Learning-Scikit%20Learn-orange)

This repository contains the code and resources for a machine learning project aimed at predicting the compatibility (`matched_score`) between job applicants and job requirements.

---

## Table of Contents
1. [Project Overview](#project-overview)
2. [Dataset](#dataset)
3. [Repository Structure](#repository-structure)
4. [Installation](#installation)
5. [Usage](#usage)
6. [Model Training and Evaluation](#model-training-and-evaluation)
7. [Contributing](#contributing)
8. [License](#license)
9. [Contact](#contact)

---

## Project Overview

The goal of this project is to predict the `matched_score`, which represents the compatibility between a job applicant's profile and the job requirements. The dataset includes features such as skills, educational background, professional experience, and job requirements.

---

## Dataset

The dataset (`train.csv`) contains the following key features:
- **Applicant Information**: Skills, educational background, professional experience, certifications, etc.
- **Job Requirements**: Educational requirements, experience requirements, skills required, etc.
- **Target Variable**: `matched_score` (a continuous value between 0 and 1).

---

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/job-applicant-matching-system.git](https://github.com/DipuHowlader/Datathon_Bitfest_2025.git)
   cd Datathon_Bitfest_2025

   # Model Training and Evaluation

This project involves the implementation and evaluation of various machine learning models for regression tasks. The models are assessed using key performance metrics to determine their effectiveness.

## Models Implemented

- **Linear Regression**
- **Random Forest Regressor**
- **Gradient Boosting Regressor**
- **LightGBM Regressor**

## Evaluation Metrics

The models are evaluated using the following metrics:

- **Mean Squared Error (MSE):** Measures the average squared difference between predicted and actual values.


## Contact

For any questions or suggestions, please feel free to reach out:

- **Email:** [dipuhowlader33@gmail.com](mailto:dipuhowlader33@gmail.com)

---

Feel free to contribute or provide feedback to help improve the project!


   
